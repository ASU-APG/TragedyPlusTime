{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "oops",
   "display_name": "oops",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from torchvision.datasets.video_utils import VideoClips, unfold\n",
    "import torch\n",
    "from torchvision.io import read_video\n",
    "from torchvision.io.video import read_video_timestamps\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = glob.glob('/mnt1/arnav/pose-estimation/oops-videos/*.mp4')\n",
    "results_dir = '/mnt1/arnav/pose-estimation/results3/'\n",
    "video_cache_path = '/mnt1/arnav/oops/r3d_feats_25fps'\n",
    "path_to_features = '/mnt1/arnav/oops/i3d_25fps_feats/rgb_feats.npy' # to verify shape\n",
    "dict_features = np.load(path_to_features, encoding='bytes', allow_pickle=True)\n",
    "mode = 'train'\n",
    "fails_data = '/mnt/arnav/oops_dataset/annotations/transition_times.json'\n",
    "fails_path = '/mnt/arnav/oops_dataset/oops_video/'\n",
    "video_mapper_path = '/mnt1/arnav/pose-estimation/mapper.json'\n",
    "fails_path = os.path.join(fails_path, mode)\n",
    "target_fps = 25\n",
    "step_between_clips_sec = 0.64\n",
    "frames_per_clip = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(video_mapper_path,'r') as f:\n",
    "    video_mapper = json.load(f)\n",
    "    reverse_video_mapper = dict((x,y) for y,x in video_mapper.items())\n",
    "video_list = glob.glob(os.path.join(f\"{fails_path}\", '**', '*.mp4'), recursive=True)\n",
    "fails_data = json.load(open(fails_data))\n",
    "video_list = [vid for vid in video_list if os.path.splitext(os.path.basename(vid))[0] in fails_data]\n",
    "new_oops_vidnames = set(np.load('../../Oops-localization/videoname.npy', allow_pickle=True).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precomputed_metadata(video_list,cache_path, mode):\n",
    "    cache_file = f\"{mode}_oops_clips.pth\"\n",
    "    cache_path = os.path.join(cache_path,cache_file)\n",
    "    print(f\"Cache dataset: True, Cache path: {cache_path}\")\n",
    "    ## load decoded clips from cache in case it is present\n",
    "    precomputed_metadata = torch.load(cache_path)\n",
    "    precomputed_metadata[\"video_paths\"] = video_list\n",
    "    return precomputed_metadata, cache_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cache dataset: True, Cache path: /mnt1/arnav/oops/r3d_feats_25fps/train_oops_clips.pth\n"
     ]
    }
   ],
   "source": [
    "precomputed_metadata, cache_path = get_precomputed_metadata(video_list, video_cache_path,mode)\n",
    "step_between_clips = round(step_between_clips_sec * target_fps)\n",
    "video_clips = VideoClips(video_list, frames_per_clip, step_between_clips, target_fps, _precomputed_metadata = precomputed_metadata)\n",
    "video_clips.compute_clips(frames_per_clip, step_between_clips, target_fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BODY_25_MAP = {\n",
    "    \"Nose\": 0,\n",
    "    \"Neck\": 1,\n",
    "    \"RShoulder\": 2,\n",
    "    \"RElbow\": 3,\n",
    "    \"RWrist\": 4,\n",
    "    \"LShoulder\": 5,\n",
    "    \"LElbow\": 6,\n",
    "    \"LWrist\": 7,\n",
    "    \"Midhip\": 8,\n",
    "    \"RHip\": 9,\n",
    "    \"RKnee\": 10,\n",
    "    \"RAnkle\": 11,\n",
    "    \"LHip\": 12,\n",
    "    \"LKnee\": 13,\n",
    "    \"LAnkle\": 14,\n",
    "    \"REye\": 15,\n",
    "    \"LEye\": 16,\n",
    "    \"REar\": 17,\n",
    "    \"LEar\": 18,\n",
    "    \"LBigToe\": 19,\n",
    "    \"LSmallToe\": 20,\n",
    "    \"LHeel\": 21,\n",
    "    \"RBigToe\": 22,\n",
    "    \"RSmallToe\": 23,\n",
    "    \"RHeel\": 24,\n",
    "    \"Background\":25\n",
    "}\n",
    "\n",
    "BODY_25_MAP = {\n",
    "    \"Nose\": 0,\n",
    "    \"Neck\": 1,\n",
    "    \"RShoulder\": 2,\n",
    "    \"RElbow\": 3,\n",
    "    \"RWrist\": 4,\n",
    "    \"LShoulder\": 5,\n",
    "    \"LElbow\": 6,\n",
    "    \"LWrist\": 7,\n",
    "    \"RHip\": 8,\n",
    "    \"RKnee\": 9,\n",
    "    \"RAnkle\": 10,\n",
    "    \"LHip\": 11,\n",
    "    \"LKnee\": 12,\n",
    "    \"LAnkle\": 13,\n",
    "    \"REye\": 14,\n",
    "    \"LEye\": 15,\n",
    "    \"REar\": 16,\n",
    "    \"LEar\": 17,\n",
    "    \"LBigToe\": 19,\n",
    "    \"LSmallToe\": 20,\n",
    "    \"LHeel\": 21,\n",
    "    \"RBigToe\": 22,\n",
    "    \"RSmallToe\": 23,\n",
    "    \"RHeel\": 24,\n",
    "    \"Background\":18\n",
    "}\n",
    "\n",
    "connections = [('Neck','LShoulder'), \n",
    "               ('Neck','RShoulder'),\n",
    "               ('LShoulder','LElbow'),\n",
    "               ('RShoulder','RElbow'),\n",
    "               ('LElbow','LWrist'),\n",
    "               ('RElbow','RWrist'),\n",
    "               ('Neck','LHip'),\n",
    "               ('Neck','RHip'),\n",
    "               ('LHip','LKnee'),\n",
    "               ('RHip','RKnee'),\n",
    "               ('LKnee','LAnkle'),\n",
    "               ('RKnee','RAnkle')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(pose, mode='body_25'):\n",
    "    if mode == 'body_25':\n",
    "        mapper = BODY_25_MAP\n",
    "    elif mode == 'coco':\n",
    "        mapper = COCO_MAP\n",
    "    pose_vector = []\n",
    "    for connection in connections:\n",
    "        c1, c2 = pose[mapper[connection[0]]], pose[mapper[connection[1]]]\n",
    "        if (c1[0]==0.0 and c1[1]==0.0) or (c2[0]==0.0 and c2[1]==0.0):\n",
    "            pose_vector.append(np.array([0,0]))\n",
    "        else:\n",
    "            den = np.sqrt( np.square(c2[0]-c1[0]) + np.square(c2[1]-c1[1]) )\n",
    "            num = np.array([c2[0]-c1[0],c2[1]-c1[1]])\n",
    "            pose_vector.append(num/den)\n",
    "    return np.array(pose_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_poses(poses, mode='body_25'):\n",
    "    poses = [vectorize(np.array(pose), mode=mode) for pose in poses] ## iterating over number of frames\n",
    "    return np.array(poses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose2vec(track_dict, top_k=1, mode='body_25'):\n",
    "    sorted_tracks = sorted(list(track_dict.items()), key=lambda x: -x[1]['hits'])[:top_k]\n",
    "    poses = [reshape_poses(sorted_tracks[i][1]['poses']) for i in range(len(sorted_tracks))] ## Iterating over number of people\n",
    "    poses = np.array(poses) # num_tracks, num_frames, num_connections, 2\n",
    "    poses = poses.reshape(poses.shape[0],poses.shape[1],-1)\n",
    "    poses = np.pad(poses, ((0,top_k-poses.shape[0]), (0,0), (0,0)), 'constant', constant_values=((0,0),(0,0),(0,0)))\n",
    "    return poses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pose_feature_dict = {}\n",
    "for video_idx, vid_clips in enumerate(video_clips.clips):\n",
    "    video_path = video_clips.video_paths[video_idx]\n",
    "    video_fps = video_clips.video_fps[video_idx]\n",
    "    video_pts = video_clips.video_pts[video_idx]\n",
    "    videoname = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    if videoname in new_oops_vidnames:\n",
    "        mapped_vidname = reverse_video_mapper[videoname]\n",
    "        total_frames = int(math.floor(len(video_pts) * (float(target_fps) / video_fps)))\n",
    "        idxs = VideoClips._resample_video_idx(total_frames, video_fps, target_fps)\n",
    "        with open(os.path.join(results_dir,mapped_vidname,'track_dict.json'),'r') as f:\n",
    "            vec = pose2vec(json.load(f), top_k=2,mode='coco')\n",
    "            vec = vec.reshape(vec.shape[1],-1)\n",
    "            # print(vec.shape)\n",
    "            vec = vec[idxs]\n",
    "            new_vec = torch.tensor(vec).unfold(0,frames_per_clip, step_between_clips)\n",
    "            new_vec = new_vec.reshape(new_vec.shape[0],-1).detach().cpu().numpy()\n",
    "            assert(new_vec.shape[0] == dict_features.item()[videoname].shape[0])\n",
    "            pose_feature_dict[videoname]=new_vec\n",
    "\n",
    "np.save(f'/mnt1/arnav/oops/pose_25fps_feats/{mode}_coco_feats.npy',pose_feature_dict)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}